{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in data and drop href and age columns (useless for analysis)\n",
    "df = pd.read_csv('FinalData.csv')\n",
    "df.drop(labels=['Href', 'Age'], axis=1, inplace=True)\n",
    "df.PER.describe()\n",
    "\n",
    "# Mean PER is 13.29 \n",
    "# Median PER of data is 13.2\n",
    "# Top 25% PER above 14.9\n",
    "# Max PER of data is 65.6 (OUTLIER) (INVESTIGATE!!)\n",
    "\n",
    "StanleyUmude = df.PER.idxmax()\n",
    "\n",
    "# Stanley Umude has played a total of 2 minutes in the NBA to acheive a PER of 65.6 which is unrealistic\n",
    "# His data will be excluded \n",
    "\n",
    "df.drop(StanleyUmude, inplace=True)\n",
    "\n",
    "df.PER.describe()\n",
    "\n",
    "# New max PER is 27.9 which is much more realistic\n",
    "# Minimum PER is negative somehow\n",
    "\n",
    "AlondesWilliams = df.PER.idxmin()\n",
    "\n",
    "# Alondes Williams has played a total of 5 minutes registering a PER of -20.9 which is unrealistic\n",
    "# His data will be excluded\n",
    "\n",
    "df.drop(AlondesWilliams, inplace=True)\n",
    "\n",
    "DariusJO = df.PER.idxmin()\n",
    "\n",
    "# Darius Johnson-Odom played a total of 21 minutes registering a PER of -18.4 which is unrealistic\n",
    "# His data will be excluded\n",
    "\n",
    "df.drop(DariusJO, inplace=True)\n",
    "\n",
    "\n",
    "KevinMurphy = df.PER.idxmin()\n",
    "\n",
    "# Kevin Murphy played a total of 52 NBA minutes registering a PER of -4.5 which is unrealistic\n",
    "# His data will be excluded\n",
    "\n",
    "df.drop(KevinMurphy, inplace=True)\n",
    "\n",
    "ChimaMoneke = df.PER.idxmin()\n",
    "\n",
    "# Chima Moneke played 8 NBA minutes and registered a PER of -3.5 - unrealistic\n",
    "# His data will be excluded\n",
    "\n",
    "df.drop(ChimaMoneke, inplace=True)\n",
    "\n",
    "newmin = df.PER.idxmin()\n",
    "df.loc[newmin]\n",
    "\n",
    "# New min is Will Conroy who has played over 100 minutes of NBA basketball\n",
    "# His data will be included\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df.describe()\n",
    "\n",
    "# Need to look into minutes\n",
    "\n",
    "JasonCollins = df.Minutes.idxmin()\n",
    "df.loc[JasonCollins]\n",
    "\n",
    "# # 15 minutes played in college not enough - dropped\n",
    "\n",
    "df.drop(JasonCollins, inplace=True)\n",
    "\n",
    "GregStemsma = df.Minutes.idxmin()\n",
    "df.loc[GregStemsma]\n",
    "\n",
    "# Only played 27 mins - drop\n",
    "\n",
    "df.drop(GregStemsma, inplace=True)\n",
    "\n",
    "KurtThomas = df.Minutes.idxmin()\n",
    "df.loc[KurtThomas]\n",
    "# # 42 minutes, not enough\n",
    "\n",
    "df.drop(KurtThomas, inplace=True)\n",
    "\n",
    "ChrisJohnson = df.Minutes.idxmin()\n",
    "df.loc[ChrisJohnson]\n",
    "# 43 mins, not enough\n",
    "\n",
    "df.drop(ChrisJohnson, inplace=True)\n",
    "\n",
    "EdmondSumner = df.Minutes.idxmin()\n",
    "df.loc[EdmondSumner]\n",
    "# 43 mins, not enough\n",
    "\n",
    "df.drop(EdmondSumner, inplace=True)\n",
    "\n",
    "MPJ = df.Minutes.idxmin()\n",
    "df.loc[MPJ]\n",
    "\n",
    "# Michael Porter Jr. only played 53 minutes in college (due to suffering a back injury)\n",
    "# He will be excluded from the data\n",
    "\n",
    "df.drop(MPJ, inplace=True)\n",
    "df.describe()\n",
    "\n",
    "# # There are still outliers\n",
    "\n",
    "Shamet = df.Minutes.idxmin()\n",
    "df.drop(Shamet, inplace=True)\n",
    "\n",
    "JackWhite = df.Minutes.idxmin()\n",
    "df.drop(JackWhite, inplace=True)\n",
    "\n",
    "JamesWiseman = df.Minutes.idxmin()\n",
    "df.drop(JamesWiseman, inplace=True)\n",
    "\n",
    "BrianHoward = df.Minutes.idxmin()\n",
    "df.drop(BrianHoward, inplace=True)\n",
    "\n",
    "DiVincenzo = df.Minutes.idxmin()\n",
    "df.drop(DiVincenzo, inplace=True)\n",
    "\n",
    "BradLohaus = df.Minutes.idxmin()\n",
    "df.drop(BradLohaus, inplace=True)\n",
    "\n",
    "York = df.Minutes.idxmin()\n",
    "df.drop(York, inplace=True)\n",
    "\n",
    "Nazr = df.Minutes.idxmin()\n",
    "df.drop(Nazr, inplace=True)\n",
    "\n",
    "AG = df.Minutes.idxmin()\n",
    "df.drop(AG, inplace=True)\n",
    "\n",
    "SG = df.Minutes.idxmin()\n",
    "df.drop(SG, inplace=True)\n",
    "\n",
    "df.describe()\n",
    "\n",
    "# Now everyone that is included in the data played a minimum of 90 minutes in college, which is sufficient\n",
    "\n",
    "# Fixing final NaNs\n",
    "\n",
    "NAminutes = pd.isnull(df['Minutes'])\n",
    "df.loc[NAminutes]\n",
    "df.drop(df[NAminutes].index, inplace=True)\n",
    "\n",
    "NAsteals=pd.isnull(df.Steals)\n",
    "df.loc[NAsteals]\n",
    "df.drop(df[NAsteals].index, inplace=True)\n",
    "\n",
    "df.isna().any()\n",
    "\n",
    "df.drop(labels=['Names'], axis=1, inplace=True)\n",
    "\n",
    "df = df.rename(columns={'Fg%':'Fgp', '3pm':'Tpm', '3pa':'Tpa', '3p%':'Tpp', 'Ft%':'Ftp'})\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv('FinalDataNoOutliers.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Normal Distribution\n",
    "\n",
    "stats.skew(df.PER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FinalDataNoOutliers.csv')\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "sn.heatmap(corr_matrix, annot=True, linewidth = .5)\n",
    "\n",
    "# Field Goal Percentage has the highest correlation with PER at 0.4\n",
    "#     Second : Blocks (0.36)\n",
    "#     Third : Rebounds (0.35)\n",
    "# On the other hand, Three Pointers made and Three Pointers attempted has the lowest correlations with PER at -0.1 and -0.12 respectively\n",
    "\n",
    "# Other interesting correleations:\n",
    "#     Points has a 0.85 correlation with minutes, 0.85 correlation to fta, 0.97 correlation to fga, and 0.98 correlation to fgm (all logical)\n",
    "#     Negatively correlated with PER:\n",
    "#         Three Pointers Made\n",
    "#         Three Pointers Attempted\n",
    "#         Three Point Percentage (logical considering the first two)\n",
    "#         and Free throw percentage\n",
    "#         This means that for each increase in these variables, we should expect a decrease in PER\n",
    "#     Strongest Negative Correlations:\n",
    "#         Three Pointers Attempted and Field Goal Percentage have a correlation of negative 0.49\n",
    "#         Three Pointers Made and Field Goal Percentage have a correlation of negative 0.43\n",
    "#         Blocks and Three pointers Attempted have a correlation of negative 0.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change out the numbers to check data entries by PER level\n",
    "\n",
    "df = pd.read_csv('FinalDataNoOutliers.csv')\n",
    "df.PER.describe()\n",
    "df[(df.PER < 9)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tiers for levels of success:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player who won't stick around in the NBA: PER less than 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fringe roster player: PER of 9 - 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will get a roster spot: PER of 11 - 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotation Player: PER of 13 - 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Role Player: PER of 15 - 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All-Star: PER of 17 - 22 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superstar: PER of 22 or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin PER for easier analysis\n",
    "\n",
    "df.loc[df.PER.between(-10, 9, 'left'), 'Bin'] = 'x<9'\n",
    "df.loc[df.PER.between(9, 11, 'left'), 'Bin'] = '9-11'\n",
    "df.loc[df.PER.between(11, 13, 'left'), 'Bin'] = '11-13'\n",
    "df.loc[df.PER.between(13, 15, 'left'), 'Bin'] = '13-15'\n",
    "df.loc[df.PER.between(15, 17, 'left'), 'Bin'] = '15-17'\n",
    "df.loc[df.PER.between(17, 22, 'left'), 'Bin'] = '17-22'\n",
    "df.loc[df.PER.between(22, 100, 'left'), 'Bin'] = '22+'\n",
    "\n",
    "BinOrder = pd.CategoricalDtype(categories=['x<9', '9-11', '11-13', '13-15', '15-17', '17-22', '22+'], ordered=True)\n",
    "df = df\n",
    "df['Bin'] = df['Bin'].astype(BinOrder)\n",
    "\n",
    "Grouped = df.groupby('Bin')['Points'].mean()\n",
    "Grouped.plot.bar()\n",
    "plt.ylabel('Average Points (college)')\n",
    "plt.xlabel('PER Bin')\n",
    "plt.title(label='Which PER Bin Scores the Most?')\n",
    "Grouped = pd.DataFrame(Grouped)\n",
    "Grouped.Points = round(Grouped.Points, 2)\n",
    "for i in range(len(Grouped)):\n",
    "    plt.text(i, Grouped.Points[i], Grouped.Points[i], ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very clear trend - Points increase at each level of PER with a massive leap in the final bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped2 = df.groupby('Bin')['Blocks'].mean()\n",
    "Grouped2.plot.bar()\n",
    "plt.ylabel('Average Blocks (college)')\n",
    "plt.xlabel('PER Bin')\n",
    "plt.title(label='Superstars Dominate with Blocks')\n",
    "Grouped2 = pd.DataFrame(Grouped2)\n",
    "Grouped2.Blocks = round(Grouped2.Blocks, 2)\n",
    "for i in range(len(Grouped2)):\n",
    "    plt.text(i, Grouped2.Blocks[i], Grouped2.Blocks[i], ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trend is very apparent between PER and blocks - increases steadily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Players who become 'Superstars' on average get far more blocks than other players (38.66% increase from the second closest bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped3 = df.groupby('Bin')['Minutes'].mean()\n",
    "Grouped3.plot.bar()\n",
    "plt.ylabel('Average Minutes (college)')\n",
    "plt.xlabel('PER Bin')\n",
    "Grouped3 = pd.DataFrame(Grouped3)\n",
    "Grouped3.Minutes = round(Grouped3.Minutes, 2)\n",
    "for i in range(len(Grouped3)):\n",
    "    plt.text(i, Grouped3.Minutes[i], Grouped3.Minutes[i], ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive trend - weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped4 = df.groupby('Bin')['Assists'].mean()\n",
    "Grouped4.plot.bar()\n",
    "plt.ylabel('Average Assists (college)')\n",
    "plt.xlabel('PER Bin')\n",
    "plt.title(label='Positional Differences Skew the Picture with Assists')\n",
    "Grouped4 = pd.DataFrame(Grouped4)\n",
    "Grouped4.Assists = round(Grouped4.Assists, 2)\n",
    "for i in range(len(Grouped4)):\n",
    "    plt.text(i, Grouped4.Assists[i], Grouped4.Assists[i], ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear trend - likely due to positional needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped5 = df.groupby('Bin')['Steals'].mean()\n",
    "Grouped5.plot.bar()\n",
    "plt.ylabel('Average Steals (college)')\n",
    "plt.xlabel('PER Bin')\n",
    "plt.title(label='Superstars take Massive Leap with Steals')\n",
    "Grouped5 = pd.DataFrame(Grouped5)\n",
    "Grouped5.Steals = round(Grouped5.Steals, 2)\n",
    "for i in range(len(Grouped5)):\n",
    "    plt.text(i, Grouped5.Steals[i], Grouped5.Steals[i], ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upward trend between steals and PER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive leap at our highest bin of PERs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped5 = df.groupby('Bin')['Rebounds'].mean()\n",
    "Grouped5.plot.bar()\n",
    "plt.ylabel('Average Rebounds (college)')\n",
    "plt.xlabel('PER Bin')\n",
    "plt.title(label='Steady Upward Trend with Rebounds')\n",
    "Grouped5 = pd.DataFrame(Grouped5)\n",
    "Grouped5.Rebounds = round(Grouped5.Rebounds, 2)\n",
    "for i in range(len(Grouped5)):\n",
    "    plt.text(i, Grouped5.Rebounds[i], Grouped5.Rebounds[i], ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped6 = df.groupby('Bin')['Fgp'].mean()\n",
    "Grouped6.plot.bar()\n",
    "plt.ylabel('Average Field-Goal Percentage (college)')\n",
    "plt.xlabel('PER Bin')\n",
    "plt.title(label='Field-Goal Percentage Increases with PER')\n",
    "Grouped6 = pd.DataFrame(Grouped6)\n",
    "Grouped6.Fgp = round(Grouped6.Fgp, 3)\n",
    "for i in range(len(Grouped6)):\n",
    "    plt.text(i, Grouped6.Fgp[i], Grouped6.Fgp[i], ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upward trend between field goals attempted and PER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better players make more shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped6 = df.groupby('Bin')['Tpp'].mean()\n",
    "Grouped6.plot.bar()\n",
    "plt.ylabel('Average 3-Point Percentage (college)')\n",
    "plt.xlabel('PER Bin')\n",
    "plt.title(label='Unclear Trend between PER and Three-Point Percentage')\n",
    "Grouped6 = pd.DataFrame(Grouped6)\n",
    "Grouped6['Tpp'] = round(Grouped6['Tpp'], 3)\n",
    "for i in range(len(Grouped6)):\n",
    "    plt.text(i, Grouped6['Tpp'][i], Grouped6['Tpp'][i], ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strongest Negative correlation with PER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear negative trend in data until final bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superstars make more three pointers than any other bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "y, X = dmatrices('PER ~ Games+Minutes+Points+Rebounds+Assists+Steals+Blocks+Fgm+Fga+Fgp+Tpm+Tpa+Tpp+Ftm+Fta+Ftp', data = df, return_type = 'dataframe')\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df['variable'] = X.columns\n",
    "\n",
    "vif_df['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "vif_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minutes seems to be a slight issue (VIF > 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redundant variables - fgm, fga, tpm, tpa, ftm, fta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices('PER ~ Games+Points+Rebounds+Assists+Steals+Blocks+Fgp+Tpp+Ftp', data = df, return_type = 'dataframe')\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df['variable'] = X.columns\n",
    "\n",
    "vif_df['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing variables from data lowers VIFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No VIF over 5 -- GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActualPER = df.pop('PER')\n",
    "ActualPER.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Minutes', 'Fgm', 'Fga', 'Fta', 'Tpm', 'Tpa', 'Ftm', 'Bin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospects = pd.read_csv('ProspectStats.csv')\n",
    "prospect_names = prospects.Player\n",
    "prospects = prospects.drop(['Player', 'Team', 'MIN', 'FGM', 'FGA', '3PM', '3PA', 'FTM', 'FTA', 'ORB', 'DRB'], axis=1)\n",
    "prospects = prospects.rename(columns={'GP':'Games', 'PTS':'Points', 'REB':'Rebounds', 'AST':'Assists', 'STL':'Steals', 'BLK':'Blocks', 'FG%':'Fgp', '3P%':'Tpp', 'FT%':'Ftp'})\n",
    "prospects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospects = prospects.iloc[:,[0,1,5,6,7,8,2,3,4]]\n",
    "prospects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, ActualPER, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "l = LinearRegression()\n",
    "lr = GridSearchCV(l, {'fit_intercept':[True, False], 'positive':[True, False]})\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.DataFrame(lr.cv_results_)\n",
    "idx = id['rank_test_score'].idxmin()\n",
    "id.iloc[idx].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared of 0.2624 - third highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = lr.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "pd.set_option('display.max_rows', 102)\n",
    "df_prospectpred.reset_index(inplace=True)\n",
    "df_prospectpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "M = MLPRegressor()\n",
    "MLP = GridSearchCV(M, {'alpha':[.00001, .0001, .001, .01, .1], 'tol':[.00001, .0001, .001, .01, .1], })\n",
    "MLP.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.DataFrame(MLP.cv_results_)\n",
    "idx = id['rank_test_score'].idxmin()\n",
    "id.iloc[idx].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MLP.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Neural Network Regression')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = MLP.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "df_prospectpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "t = DecisionTreeRegressor()\n",
    "tree = GridSearchCV(t, {'max_depth':[1,2,3,None], 'min_samples_split': [6,7,8,9,None], 'min_samples_leaf':[7,8,9,10,None], 'ccp_alpha':[.0005,.005,.05,.1]})\n",
    "tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.DataFrame(tree.cv_results_)\n",
    "idx = id['rank_test_score'].idxmin()\n",
    "id.iloc[idx].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Decision Tree')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared of .1163 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horrible fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = tree.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "df_prospectpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "r = RandomForestRegressor()\n",
    "rf = GridSearchCV(r, {'n_estimators':[50,100,200,300], 'max_depth':[None,75,100,200], 'min_samples_split':[2,3,4,5], 'min_samples_leaf':[1,3,5,7,9]})\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.DataFrame(rf.cv_results_)\n",
    "idx = id['rank_test_score'].idxmin()\n",
    "id.iloc[idx].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Random Forest Regression')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue with random forest: two distinct groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One group that is underpredicted and one that is overpredicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model doesn't seem to predict very high or very low PERs (Most predicted values are between 10 and 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared of 0.2247 - better than decision tree, but not in the top 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = rf.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "df_prospectpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, model_selection\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "k = KNeighborsRegressor()\n",
    "knn = GridSearchCV(k, {'n_neighbors':[3,5,10,40], 'p':[1,2,5,7,10], 'leaf_size':[1,4,7,10,20,30]})\n",
    "knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.DataFrame(knn.cv_results_)\n",
    "idx = id['rank_test_score'].idxmin()\n",
    "id.iloc[idx].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('KNN Regression')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared of .1503 - not great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = knn.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "df_prospectpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "s = SVR()\n",
    "svr = GridSearchCV(s, {'degree':[1,2,3,4], 'gamma':[.005,.01,.05,.1], 'tol':[.0001, .001, .01, .1], 'C':[1,3,5,7,9]})\n",
    "svr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.DataFrame(svr.cv_results_)\n",
    "idx = id['rank_test_score'].idxmin()\n",
    "id.iloc[idx].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svr.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('SVR Regression')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horrible - .0162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = svr.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "df_prospectpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "x = GradientBoostingRegressor()\n",
    "xgb = GridSearchCV(x, {'learning_rate':[.05,.1,.3],'n_estimators':[10,50,100], 'min_samples_split':[2,3], 'min_weight_fraction_leaf':[0,.05,.1,.2], 'max_depth':[5,6], 'alpha':[.1,.3,], 'tol':[.0001, .001]})\n",
    "xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(xgb.cv_results_)\n",
    "minres = res['rank_test_score'].idxmin()\n",
    "res.iloc[minres].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('XGBoost Regression')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fourth best - .2417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = xgb.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "df_prospectpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rid = BayesianRidge()\n",
    "ridge = GridSearchCV(rid, {'tol':[.0001, .001, .01, .1], 'alpha_1':[.000001, .00001, .0001, .001, .01, .1], 'alpha_2':[.000001, .00001, .0001, .001, .01, .1], 'lambda_1':[.000001, .00001, .0001, .001, .01, .1], 'lambda_2':[.000001, .00001, .0001, .001, .01, .1]})\n",
    "ridge.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(ridge.cv_results_)\n",
    "minres = res['rank_test_score'].idxmin()\n",
    "res.iloc[minres].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ridge.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Bayesian Ridge Regression')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fifth best - .2366"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = ridge.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "df_prospectpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "las = Lasso()\n",
    "lasso = GridSearchCV(las, {'alpha':[.0001,.001,.01,.1,1.0,2.0], 'tol':[.0001,.001,.01,.1]})\n",
    "lasso.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(lasso.cv_results_)\n",
    "minres = res['rank_test_score'].idxmin()\n",
    "res.iloc[minres].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Lasso Regression')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second best - .2726"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = lasso.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "pd.set_option('display.max_rows', 102)\n",
    "df_prospectpred.reset_index(inplace=True)\n",
    "df_prospectpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "e = ElasticNet()\n",
    "en = GridSearchCV(e, {'alpha':[.0001,.001,.01,.1], 'l1_ratio':[.01,.1,.5,.9], 'tol':[.0001,.001,.01,.1,.5,.9]})\n",
    "en.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(en.cv_results_)\n",
    "minres = res['rank_test_score'].idxmin()\n",
    "res.iloc[minres].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = en.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['yPredict'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(y_test, y_pred, label = 'Predicted vs Actual PERs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Elastic Net Regression')\n",
    "plt.xlabel('Actual PER')\n",
    "plt.ylabel('Predicted PER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.r2_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST - .2729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = en.predict(prospects)\n",
    "p_pred = pd.DataFrame(p_pred, columns=['pPredict'])\n",
    "df_prospectpred = pd.concat([prospect_names,p_pred], axis=1)\n",
    "df_prospectpred.sort_values('pPredict', ascending=False, inplace=True)\n",
    "pd.set_option('display.max_rows', 102)\n",
    "df_prospectpred.reset_index(inplace=True)\n",
    "df_prospectpred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
